{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe71913b-d16c-4c06-8e13-7a43b88d6e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "current_time = datetime.now().isoformat()\n",
    "ingest_type = \"stream\"\n",
    "path = f\"/Volumes/capstone/schema_bronze/00_data/\"\n",
    "ingest_path = path+ingest_type\n",
    "archive_path = path + \"archive/\"\n",
    "file_list = [file for file in dbutils.fs.ls(ingest_path)]\n",
    "catalog_schema = \"capstone.schema_bronze\"\n",
    "\n",
    "file_list\n",
    "\n",
    "if file_list is None:\n",
    "    log_run_history={\n",
    "                \"file_name\": \"\",\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"SKIP\",\n",
    "                \"type\":\"No file to process\",\n",
    "                \"error_message\": \"\"\n",
    "                }\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52341a55-7869-4781-adab-bdb56d83b2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#this is autoloader for streaming ingest and transform. But as I use free version is was unabled. SO, I have to comment this out and use mock script\n",
    "\n",
    "# # Read source table as stream\n",
    "# source_df = (\n",
    "#     spark.readStream\n",
    "#     .format(\"delta\")\n",
    "#     .table(\"source_table\")\n",
    "# )\n",
    "\n",
    "# # Apply transformation logic\n",
    "# transformed_df = source_df.filter(col(\"status\") == \"active\")\n",
    "\n",
    "# # Write into target table\n",
    "# (\n",
    "#     transformed_df.writeStream\n",
    "#     .format(\"delta\")\n",
    "#     .option(\"checkpointLocation\", \"/mnt/checkpoints/myjob\")\n",
    "#     .outputMode(\"append\")  # append only new rows\n",
    "#     .table(\"target_table\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767b449a-45e6-49b6-a0ea-a8321539f872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    \n",
    "    file_path = file.path\n",
    "    table_full = catalog_schema + \".\" + file.name.split(\".\")[0]\n",
    "    file_type = file.name.split(\".\")[-1]\n",
    "\n",
    "    # Check if file exists before processing\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        file_exists = dbutils.fs.ls(file_path)\n",
    "        print(f\"{file.name} exists\")\n",
    "\n",
    "    except Exception as file_check_error:\n",
    "        # Handle file check errors (e.g., path doesn't exist)\n",
    "        if \"java.io.FileNotFoundException\" in str(file_check_error) or \"Path does not exist\" in str(file_check_error):\n",
    "            log_run_history={\n",
    "                \"file_name\": file.name,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"FILE_NOT_FOUND\",\n",
    "                \"type\":\"stream file or directory not found\",\n",
    "                \"error_message\": str(file_check_error),\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            # Other file system errors\n",
    "            log_run_history={\n",
    "                \"file_name\": file.name,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"ERROR\",\n",
    "                \"type\":\"Error occurred during file system check\",\n",
    "                \"error_message\": str(file_check_error),\n",
    "                }\n",
    "            \n",
    "        file_exists = None\n",
    "        print(log_run_history[\"type\"])\n",
    "\n",
    "    # File exists, proceed with processing\n",
    "    if file_exists is not None:\n",
    "        print(f\"processing {file.name}........\")\n",
    "        try:\n",
    "            \n",
    "            # Process the file from volume\n",
    "            if file_type == \"csv\":\n",
    "                claims_stream_df = (\n",
    "                    spark.read\n",
    "                        .option(\"header\", \"true\")\n",
    "                        .csv(file_path)   # path to stream file\n",
    "                        .withColumn(\"Amount\", col(\"Amount\").cast(\"string\"))\n",
    "                        .withColumn(\"_source\", lit(ingest_type))\n",
    "                        .withColumn(\"_ingestion_timestamp\", lit(current_time).cast(\"timestamp\"))\n",
    "                    )\n",
    "            elif file_type == \"json\":\n",
    "                claims_stream_df = (\n",
    "                    spark.read\n",
    "                        .option(\"header\", \"true\")\n",
    "                        .json(file_path)   # path to stream file\n",
    "                        .withColumn(\"Amount\", col(\"Amount\").cast(\"string\"))\n",
    "                        .withColumn(\"_source\", lit(ingest_type))\n",
    "                        .withColumn(\"_ingestion_timestamp\", lit(current_time).cast(\"timestamp\"))\n",
    "                    )\n",
    "            \n",
    "            # Get record count for logging\n",
    "            record_count = claims_stream_df.count()\n",
    "            \n",
    "            # Write to table\n",
    "            claims_stream_df.write.mode(\"overwrite\").saveAsTable(table_full)\n",
    "            \n",
    "            # Move and rename the file after processed\n",
    "            new_file_name = str(current_time) + \"_\" + file.name\n",
    "            new_file_path = archive_path + new_file_name\n",
    "            dbutils.fs.mv(file_path, new_file_path)\n",
    "            \n",
    "            # Log success\n",
    "            log_run_history={\n",
    "                \"file_name\": file.name,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"SUCCESS\",\n",
    "                \"type\":f\"{ingest_type} file processed successfully\",\n",
    "                \"error_message\": \"\",\n",
    "                }\n",
    "\n",
    "            \n",
    "        except Exception as processing_error:\n",
    "            # Log processing error\n",
    "            log_run_history={\n",
    "                \"file_name\": file.name,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"ERROR\",\n",
    "                \"type\":\"Error occurred during file processing\",\n",
    "                \"error_message\": str(processing_error),\n",
    "                }\n",
    "            # print(processing_error)\n",
    "        print(log_run_history[\"type\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f999b83-c5e3-4569-ad40-575c1b37245b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_log = spark.createDataFrame([log_run_history])\n",
    "df_log.write.mode(\"append\").saveAsTable(\"capstone.schema_bronze.log_ingest\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_claim_stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
