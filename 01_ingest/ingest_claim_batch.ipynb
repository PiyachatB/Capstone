{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686c74f3-9410-4e6b-9d01-cf147da0b2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "current_time = datetime.now().isoformat()\n",
    "ingest_type = \"batch\"\n",
    "path = f\"/Volumes/capstone/schema_bronze/00_data/{ingest_type}/\"\n",
    "archive_path = path + \"archive/\"\n",
    "file_list = [file.name for file in dbutils.fs.ls(path) if file.name != \"archive/\"]\n",
    "catalog_schema = \"capstone.schema_bronze\"\n",
    "\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc1a49e2-b6ad-4935-9a9f-910fd4d0112a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    \n",
    "    file_path = path + file\n",
    "    table_full = catalog_schema + \".\" + file.split(\".\")[0]\n",
    "\n",
    "    # Check if file exists before processing\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        file_exists = dbutils.fs.ls(file_path)\n",
    "        print(f\"{file} exists\")\n",
    "\n",
    "    except Exception as file_check_error:\n",
    "        # Handle file check errors (e.g., path doesn't exist)\n",
    "        if \"java.io.FileNotFoundException\" in str(file_check_error) or \"Path does not exist\" in str(file_check_error):\n",
    "            log_run_history={\n",
    "                \"file_name\": file,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"NO_FILE\",\n",
    "                \"type\":\"Batch file or directory not found\",\n",
    "                \"error_message\": str(file_check_error),\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            # Other file system errors\n",
    "            log_run_history={\n",
    "                \"file_name\": file,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"ERROR\",\n",
    "                \"type\":\"Error occurred during file system check\",\n",
    "                \"error_message\": str(file_check_error),\n",
    "                }\n",
    "            \n",
    "        file_exists = None\n",
    "        print(log_run_history[\"type\"])\n",
    "\n",
    "    # File exists, proceed with processing\n",
    "    if file_exists is not None:\n",
    "        print(f\"processing {file}........\")\n",
    "        try:\n",
    "            \n",
    "            # Process the file from volume\n",
    "            claims_batch_df = (\n",
    "                spark.read\n",
    "                    .option(\"header\", \"true\")\n",
    "                    .csv(file_path)   # path to batch file\n",
    "                    .withColumn(\"_source\", lit(ingest_type))\n",
    "                    .withColumn(\"_ingestion_timestamp\", lit(current_time).cast(\"timestamp\"))\n",
    "            )\n",
    "            \n",
    "            # Get record count for logging\n",
    "            record_count = claims_batch_df.count()\n",
    "            \n",
    "            # Write to table\n",
    "            claims_batch_df.write.mode(\"overwrite\").saveAsTable(table_full)\n",
    "            \n",
    "            # Move and rename the file after processed\n",
    "            new_file_name = str(current_time) + \"_\" + file\n",
    "            new_file_path = archive_path + new_file_name\n",
    "            dbutils.fs.mv(file_path, new_file_path)\n",
    "            \n",
    "            # Log success\n",
    "            log_run_history={\n",
    "                \"file_name\": file,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"SUCCESS\",\n",
    "                \"type\":f\"{ingest_type} file processed successfully\",\n",
    "                \"error_message\": None,\n",
    "                }\n",
    "\n",
    "            \n",
    "        except Exception as processing_error:\n",
    "            # Log processing error\n",
    "            log_run_history={\n",
    "                \"file_name\": file,\n",
    "                \"timestamp\": current_time,\n",
    "                \"status\":\"ERROR\",\n",
    "                \"type\":\"Error occurred during file processing\",\n",
    "                \"error_message\": str(processing_error),\n",
    "                }\n",
    "            print(processing_error)\n",
    "        print(log_run_history[\"type\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24c01db-12e3-461e-b98a-77fae4a7b91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_run_history"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_claim_batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
